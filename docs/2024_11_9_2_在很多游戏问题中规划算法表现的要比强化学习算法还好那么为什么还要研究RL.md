---

title: 在很多游戏问题中规划算法表现的要比强化学习算法还好,那么为什么还要研究RL
 
description: 

#多个标签请使用英文逗号分隔或使用数组语法

tags: 杂谈

#多个分类请使用英文逗号分隔或使用数组语法，暂不支持多级分类
---

根据前段时间分享的对一些游戏，如《俄罗斯方块》、《贪吃蛇》、《2048》游戏上来看，可以知道一个精调好的规划算法（启发式算法），在人为给定的一些预设条件下运行，其最终的算法性能会比一般的RL算法实现的效果要好，但是为什么我们还要研究RL算法呢，那么是不是说明RL算法这种AI算法就没有太多的意义来用来打游戏了呢？



本篇是解惑blog。



其实，通过之前的blogs的分析我们可以发现，使用启发式算法确实在形式上更加的简单，并且不需要像AI算法那样花费长时间进行算法模型的训练，而且在算法的最终表现上甚至要优于RL算法，但是我们也可以清楚的发现在这些启发式的规划算法中我们对于这些条件的设定其实都是所谓的来自于专家角度的，也就是说比如规划法解《俄罗斯方块》尤其，如何判定哪个action之后的next observation更好，这其中的cost函数所包括的各个因素（积木块堆积的高度、凹凸度，等等）都是通过专家给定的，并且这些因素的权重也是所谓的专家优化和调整过的，可以看到在启发式算法中最重要的就是专家经验，而且这个专家经验是根据具体游戏的不同而不同的，就比如在《俄罗斯方块》、《贪吃蛇》、《2048》游戏上，就需要三个完全不相关的不同的专家经验来给出启发式条件设定的。总的来说，启发式算法的求解效果是建立在有很好专家经验的基础上的，虽然启发式算法的不需要训练和求解效果极好的表现都是AI算法无法比拟的，但是这种解法并不通用，也就是说对于10000个游戏就需要10000个启发式算法和启发式条件，而这就意味着需要10000个专家经验来对启发式条件微调，但是AI算法是通用的，就比如RL中的DQN算法解决atari 2600游戏一样，我们可以在小幅度调整甚至可以不需要调整的情况下直接使用DQN算法来对这些游戏进行求解。而且要知道，我们使用AI算法求解学术中的问题并不是以此为终点的，而是要在此基础上可以泛化到或者说推广到其他的实际问题中的，而启发式算法由于其解决问题的这种特定性是不具备泛化性和向外推广性的，因此我们为了追求更加通用的求解方法所以才会需要研究AI算法。



使用启发式算法是同样可以求解atari游戏的，比如我们可以根据不同的游戏预先设定启发规则，比如设置当敌人到达某位置后我们发射子弹等等，但是这样并不能得到通用型的智能算法；再比如在cartpole游戏中，在小车平衡杆问题下，我们使用物理公式进行数学解析式下的建模求解就可以获得极好的解法，这是远比RL算法要有更好表现的，但是这种解法是需要针对特定问题进行物理建模再转成数学解析的，这是无法通用推广并泛化到其他问题中的，而我们使用DQN或A3C或PPO算法来解决cart-pole问题同样也可以应用到其他问题中的。



<br/>

<br/>

**使用启发式算法求解《贪吃蛇》游戏的视频：**

[I Created a PERFECT SNAKE A.I.](https://www.youtube.com/watch?v=tjQIO1rqTBE)



汉密尔顿法求解《贪吃蛇》简直封神了，可以说这个表现是AI算法难以匹敌的，不过这个解法也就只能用来解决《贪吃蛇》游戏了。



**使用强化学习算法求解《贪吃蛇》游戏的视频：**

[A.I. Learns to play Snake using Deep Q Learning](https://www.youtube.com/watch?v=-NJ9frfAWRo)



可以看到，使用启发式算法求解《贪吃蛇》需要专家不断调整算法的设计和启发规则，其主要耗费的是人脑，而强化学习算法求解《贪吃蛇》主要耗费的是计算机的计算资源。





<br/>

强化学习算法library库：(集成库)

https://github.com/Denys88/rl_games



https://github.com/Domattee/gymTouch







**个人github博客地址：**
[https://devilmaycry812839668.github.io/](https://devilmaycry812839668.github.io/ "https://devilmaycry812839668.github.io/")