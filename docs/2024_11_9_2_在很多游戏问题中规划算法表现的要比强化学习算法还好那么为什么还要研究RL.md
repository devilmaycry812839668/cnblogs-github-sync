---

title: 在很多游戏问题中规划算法表现的要比强化学习算法还好,那么为什么还要研究RL
 
description: 

#多个标签请使用英文逗号分隔或使用数组语法

tags: 杂谈

#多个分类请使用英文逗号分隔或使用数组语法，暂不支持多级分类
---

根据前段时间分享的对一些游戏，如《俄罗斯方块》、《贪吃蛇》、《2048》游戏上来看，可以知道一个精调好的规划算法（启发式算法），在人为给定的一些预设条件下运行，其最终的算法性能会比一般的RL算法实现的效果要好，但是为什么我们还要研究RL算法呢，那么是不是说明RL算法这种AI算法就没有太多的意义来用来打游戏了呢？



本篇是解惑blog。



其实，通过之前的blogs的分析我们可以发现，使用启发式算法确实在形式上更加的简单，并且不需要像AI算法那样花费长时间进行算法模型的训练，而且在算法的最终表现上甚至要优于RL算法，但是我们也可以清楚的发现在这些启发式的规划算法中我们对于这些条件的设定其实都是所谓的来自于专家角度的，也就是说比如规划法解《俄罗斯方块》尤其，如何判定哪个action之后的next observation更好，这其中的cost函数所包括的各个因素（积木块堆积的高度、凹凸度，等等）都是通过专家给定的，并且这些因素的权重也是所谓的专家优化和调整过的，可以看到在启发式算法中最重要的就是专家经验，而且这个专家经验是根据具体游戏的不同而不同的，就比如在《俄罗斯方块》、《贪吃蛇》、《2048》游戏上，就需要三个完全不相关的不同的专家经验来给出启发式条件设定的。总的来说，启发式算法的求解效果是建立在有很好专家经验的基础上的，虽然启发式算法的不需要训练和求解效果极好的表现都是AI算法无法比拟的，但是这种解法并不通用，也就是说对于10000个游戏就需要10000个启发式算法和启发式条件，而这就意味着需要10000个专家经验来对启发式条件微调，但是AI算法是通用的，就比如RL中的DQN算法解决atari 2600游戏一样，我们可以在小幅度调整甚至可以不需要调整的情况下直接使用DQN算法来对这些游戏进行求解。而且要知道，我们使用AI算法求解学术中的问题并不是以此为终点的，而是要在此基础上可以泛化到或者说推广到其他的实际问题中的，而启发式算法由于其解决问题的这种特定性是不具备泛化性和向外推广性的，因此我们为了追求更加通用的求解方法所以才会需要研究AI算法。



使用启发式算法是同样可以求解atari游戏的，比如我们可以根据不同的游戏预先设定启发规则，比如设置当敌人到达某位置后我们发射子弹等等，但是这样并不能得到通用型的智能算法；再比如在cartpole游戏中，在小车平衡杆问题下，我们使用物理公式进行数学解析式下的建模求解就可以获得极好的解法，这是远比RL算法要有更好表现的，但是这种解法是需要针对特定问题进行物理建模再转成数学解析的，这是无法通用推广并泛化到其他问题中的，而我们使用DQN或A3C或PPO算法来解决cart-pole问题同样也可以应用到其他问题中的。



<br/>

<br/>

**使用启发式算法求解《贪吃蛇》游戏的视频：**

[I Created a PERFECT SNAKE A.I.](https://www.youtube.com/watch?v=tjQIO1rqTBE)



汉密尔顿法求解《贪吃蛇》简直封神了，可以说这个表现是AI算法难以匹敌的，不过这个解法也就只能用来解决《贪吃蛇》游戏了。



**使用强化学习算法求解《贪吃蛇》游戏的视频：**

[A.I. Learns to play Snake using Deep Q Learning](https://www.youtube.com/watch?v=-NJ9frfAWRo)



可以看到，使用启发式算法求解《贪吃蛇》需要专家不断调整算法的设计和启发规则，其主要耗费的是人脑，而强化学习算法求解《贪吃蛇》主要耗费的是计算机的计算资源。

<br/>

不过要说明下，虽然像《贪吃蛇》这样的游戏，启发式算法的性能是AI算法的性能所无法比拟的，或者说甚至一些可以在数学上得到解析解的问题最直接最简单最高效的求解方法就是使用解析解，如：贪吃蛇、cart-pole游戏，但是为了研究问题的泛化性我们依旧是要以研究AI算法为重大方向的，因为启发式算法不具备通用性，或者说启发式算法局限性很大，虽然像$A^*$这样的算法有巨大的应用场景，但是一般来说在主要场景性下都显得简单和有效，因此在很多时候启发式算法会被用作是baseline算法。



虽然AI算法解决《贪吃蛇》游戏问题效果不如启发式算法，但是为了更好的理解AI算法在该问题上的应用，我们在给出RL算法求解《贪吃蛇》问题以外给出使用遗传算法求解该问题的解法：

[AI Learns to play Snake!](https://www.youtube.com/watch?v=vhiO4WsHA6c)

根据游戏的特性我们可以知道《贪吃蛇》游戏其实是一个奖励稀疏的问题，因为我们一般都是建立奖励函数为吃到苹果后得到100 reward，但是这样的稀疏奖励问题是不利于强化学习算法训练的，或者说稀疏奖励的情况下强化学习的训练效率是十分低的，为此我们往往可以通过其他的奖励函数的设置实现稀疏问题向非稀疏问题的转换，比如：我们可以设置得到苹果的reward为10000，而蛇每走一步的奖励为+1，但是这样的奖励函数更加可能的是训练出一个一直在转圈的蛇，这样这个蛇可以通过不断的转圈来不停的得到reward为1的奖励，这样反而会导致蛇在增长到一定长度后不去吃苹果反而开始转圈的问题，为解决这个问题我们可以通过调整每一步的reward的大小，甚至可以在蛇开始转圈的时候改变奖励函数，比如在蛇开始转圈的时候将每step的reward从+1变为+0，不过即使这样也不能很好的抑制RL算法在这种稀疏奖励问题上的低效性，而设置一个可以将稀疏reward变为非稀疏reward的奖励函数又是比较难的，比如我们可以设置一个reward项来评价蛇能否走一个最远的距离都是不转圈的reward，这时候遗传算法就可以有一个比较好的应用了。



因为RL算法难以解决《贪吃蛇》游戏中的稀疏reward问题，而遗传算法则不需要对每一个step作评价，而是在游戏结束时给出一个fitness函数用来对整个游戏过程进行评估，因此使用遗传算法可以很好的规避掉稀疏奖励问题，这里给出遗传算法的求解方法：

[AI Learns to play Snake!](https://www.youtube.com/watch?v=vhiO4WsHA6c)



<br/>

强化学习算法library库：(集成库)

https://github.com/Denys88/rl_games



https://github.com/Domattee/gymTouch







**个人github博客地址：**
[https://devilmaycry812839668.github.io/](https://devilmaycry812839668.github.io/ "https://devilmaycry812839668.github.io/")