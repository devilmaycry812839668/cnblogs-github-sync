---

title: 模型压缩后的强化学习模型性能是否会受影响
 
description: 

#多个标签请使用英文逗号分隔或使用数组语法

tags: 杂谈

#多个分类请使用英文逗号分隔或使用数组语法，暂不支持多级分类
---



最近看了一些关于CV领域的模型压缩的论文，突然想到了这个问题，那就是模型压缩后的强化学习模型性能是否会受影响。



模型压缩是一个伴随深度学习的老问题了，这个问题一直都是CV领域的，不过这两年随着NLP的大模型的火爆也成了NLP的一个热点问题了，但是由于我是做RL方向的，于是我就想到了本文的主体，那就是模型压缩后的强化学习模型性能是否会受影响。



<br/>

总所周知的一个事情，那就是在CV和NLP领域，使用模型压缩后虽然可以得到更小体量的模型，但是势必会一定程度上降低算法性能，但是只要这种性能下降的程度可以接受就可以采用，并且这种小体量的模型是有可能运行在移动设备上的，否则大模型是难以直接运行在移动设备上的，当然我们也可以使用大模型运行在云上的解决方案，不过这样的会就需要使用低时延的网络了，如：5G网络。



按照相关的其他领域的论文来推测，使用模型压缩后的强化学习算法模型也必然会降低算法性能，但是这个下降程度会如何，是否可以控制在可接受范围，或者说模型压缩是否会导致强化学习模型的性能完全崩溃，这些问题也是没有看到具体的研究的research paper的，本文在这里也只是提出个疑问，而不是是实际研究操作的。



我的一个大胆猜测，那就是使用模型压缩后的强化学习模型，其性能的下降幅度是有可能高于CV和NLP的，因为我个人任务RL的算法模型是更脆弱的，因为RL的算法模型更难训练，那么得到的算法模型也必然更脆弱，更容易被破坏其稳定性。当然，这里由于没有时间去做实际的实验来探究，因此也只是个设想和猜测。









<br/>

强化学习算法library库：(集成库)

https://github.com/Denys88/rl_games



https://github.com/Domattee/gymTouch







**个人github博客地址：**
[https://devilmaycry812839668.github.io/](https://devilmaycry812839668.github.io/ "https://devilmaycry812839668.github.io/")